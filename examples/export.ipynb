{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51769646",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
    "\n",
>>>>>>> 61e1baa5c ([chore] end of the tutorial)
    "# Turn Your AI Into GBA Code!\n",
    "\n",
    "Ever dreamed of running your own neural network on a real Game Boy Advance? That’s exactly what this project lets you do—convert your ONNX model into C code that runs right inside Pokémon Emerald!\n",
    "\n",
    "Last time, we trained a model. Now, let’s bring it to life on the GBA.\n",
    "\n",
<<<<<<< HEAD
    "## What You’ll Do in This Tutorial\n",
    "- Load your trained ONNX model  \n",
    "- Quantize it to full int8 for retro hardware  \n",
    "- Run and test it directly in the emulator  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f127d",
   "metadata": {},
   "source": [
    "### Imports"
=======
    "# Generate a C export for the GBA\n",
    "If you have trained model in onnx, you can generate a c export of inference for the gba "
=======
    "## What You’ll Do in This Tutorial \n",
    "- Load your trained ONNX model\n",
    "- Quantize it to full int8 for retro hardware\n",
    "- Run and test it directly in the emulator\n",
    "\n",
    "### Imports"
>>>>>>> 61e1baa5c ([chore] end of the tutorial)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2d3e066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnx import shape_inference\n",
    "from onnx import numpy_helper\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pkmn_rl_arena.quantize.quantize import FullQuantizer\n",
    "from pkmn_rl_arena.export.onnx_exporter import ONNXExporter\n",
    "from pkmn_rl_arena.paths import PATHS\n",
    "\n",
    "from pkmn_rl_arena.export.passes.delete_pass import (\n",
    "    DeleteFirstInputQDQPass,\n",
    "    DeleteQuantizePass,\n",
    "    DeleteFirstLastQuantizeDequantizePass,\n",
    ")\n",
    "from pkmn_rl_arena.export.passes.fusion_pass import (\n",
    "    GemmQuantDequantFusionPass,\n",
    ")\n",
    "\n",
    "from pkmn_rl_arena.export.base import ExportBaseGba\n",
    "from pkmn_rl_arena.export.onnx_utils import OnnxUtils\n",
    "from pkmn_rl_arena.data.parser import MapAnalyzer\n",
    "\n",
    "from pkmn_rl_arena.export.exporters.parameters import ExportParameters\n",
    "import rustboyadvance_py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe0918",
   "metadata": {},
   "source": [
    "### Set Model Paths\n",
    "\n",
    "Before moving forward, let's organize the three key ONNX files we'll use:\n",
    "\n",
    "- **Original ONNX:** The model you trained, in full precision.\n",
    "- **Quantized ONNX:** Converted to int8 for hardware compatibility, and used to test inference with ONNX Runtime.\n",
    "- **Fused ONNX:** Created from the quantized model, with Gemm and QDQ operations fused together so our export module can process it for the GBA.\n",
    "\n",
    "Defining these paths now will make the next steps much smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4990bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"pokemon_battle_model.onnx\"\n",
    "quantized_onnx_path = \"pokemon_battle_model_quantized.onnx\"\n",
    "fused_path = \"pokemon_battle_model_quantized_fused.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf4b16",
   "metadata": {},
   "source": [
    "You can print the actual graph with ONNX to see the changes we make.  \n",
    "For a clearer visualization, try Netron—a handy tool for exploring ONNX model graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c622af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %input[FLOAT, 1x360]\n",
      ") initializers (\n",
      "  %w0[FLOAT, 360x128]\n",
      "  %b0[FLOAT, 128]\n",
      "  %w1[FLOAT, 128x10]\n",
      "  %b1[FLOAT, 10]\n",
      ") {\n",
      "  %/Add_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 0](%input, %w0, %b0)\n",
      "  %/Relu_output_0 = Relu(%/Add_output_0)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transA = 0, transB = 0](%/Relu_output_0, %w1, %b1)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "onnx_raw = onnx.load(onnx_path)\n",
    "print(onnx.helper.printable_graph(onnx_raw.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964fd6b",
   "metadata": {},
   "source": [
    "## Quantize the model\n",
    "\n",
    "Quantization is the process of converting your model’s weights and activations from floating point to int8, making it much more efficient for hardware like the GBA.  \n",
    "\n",
    "To do this, we need to calibrate QDQ (Quantize-Dequantize) pairs using sample data, which helps the model learn how to scale values correctly.\n",
    "\n",
    "> Note:  \n",
    "> Calibration works best with real data from your application. In future versions, we’ll provide a tool to help generate calibration data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec996992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %input[FLOAT, 1x360]\n",
      ") initializers (\n",
      "  %input_zero_point[INT8, scalar]\n",
      "  %input_scale[FLOAT, scalar]\n",
      "  %/Add_output_0_zero_point[INT8, scalar]\n",
      "  %/Add_output_0_scale[FLOAT, scalar]\n",
      "  %w0_zero_point[INT8, scalar]\n",
      "  %w0_scale[FLOAT, scalar]\n",
      "  %w0_quantized[INT8, 360x128]\n",
      "  %/Relu_output_0_zero_point[INT8, scalar]\n",
      "  %/Relu_output_0_scale[FLOAT, scalar]\n",
      "  %output_zero_point[INT8, scalar]\n",
      "  %output_scale[FLOAT, scalar]\n",
      "  %w1_zero_point[INT8, scalar]\n",
      "  %w1_scale[FLOAT, scalar]\n",
      "  %w1_quantized[INT8, 128x10]\n",
      "  %b0_quantized[INT32, 128]\n",
      "  %b0_quantized_scale[FLOAT, 1]\n",
      "  %b0_quantized_zero_point[INT32, scalar]\n",
      "  %b1_quantized[INT32, 10]\n",
      "  %b1_quantized_scale[FLOAT, 1]\n",
      "  %b1_quantized_zero_point[INT32, scalar]\n",
      ") {\n",
      "  %b0 = DequantizeLinear(%b0_quantized, %b0_quantized_scale, %b0_quantized_zero_point)\n",
      "  %b1 = DequantizeLinear(%b1_quantized, %b1_quantized_scale, %b1_quantized_zero_point)\n",
      "  %input_QuantizeLinear_Output = QuantizeLinear(%input, %input_scale, %input_zero_point)\n",
      "  %w0_DequantizeLinear_Output = DequantizeLinear(%w0_quantized, %w0_scale, %w0_zero_point)\n",
      "  %w1_DequantizeLinear_Output = DequantizeLinear(%w1_quantized, %w1_scale, %w1_zero_point)\n",
      "  %input_DequantizeLinear_Output = DequantizeLinear(%input_QuantizeLinear_Output, %input_scale, %input_zero_point)\n",
      "  %/Add_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 0](%input_DequantizeLinear_Output, %w0_DequantizeLinear_Output, %b0)\n",
      "  %/Add_output_0_QuantizeLinear_Output = QuantizeLinear(%/Add_output_0, %/Add_output_0_scale, %/Add_output_0_zero_point)\n",
      "  %/Add_output_0_DequantizeLinear_Output = DequantizeLinear(%/Add_output_0_QuantizeLinear_Output, %/Add_output_0_scale, %/Add_output_0_zero_point)\n",
      "  %/Relu_output_0 = Relu(%/Add_output_0_DequantizeLinear_Output)\n",
      "  %/Relu_output_0_QuantizeLinear_Output = QuantizeLinear(%/Relu_output_0, %/Relu_output_0_scale, %/Relu_output_0_zero_point)\n",
      "  %/Relu_output_0_DequantizeLinear_Output = DequantizeLinear(%/Relu_output_0_QuantizeLinear_Output, %/Relu_output_0_scale, %/Relu_output_0_zero_point)\n",
      "  %output_QuantizeLinear_Input = Gemm[alpha = 1, beta = 1, transA = 0, transB = 0](%/Relu_output_0_DequantizeLinear_Output, %w1_DequantizeLinear_Output, %b1)\n",
      "  %output_QuantizeLinear_Output = QuantizeLinear(%output_QuantizeLinear_Input, %output_scale, %output_zero_point)\n",
      "  %output = DequantizeLinear(%output_QuantizeLinear_Output, %output_scale, %output_zero_point)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_samples = 10\n",
    "\n",
    "quantizer = FullQuantizer(onnx_path, quantized_onnx_path)\n",
    "calib_reader = FullQuantizer.create_fake_calibration_data(\n",
    "    onnx_path, num_samples=num_samples\n",
    ")\n",
    "quantizer.quantize(calib_reader)\n",
    "\n",
    "# Infer shapes\n",
    "quantized_model = onnx.load(quantized_onnx_path)\n",
    "inferred_model = shape_inference.infer_shapes(quantized_model)\n",
    "onnx.save(inferred_model, quantized_onnx_path)\n",
    "\n",
    "print(onnx.helper.printable_graph(inferred_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f214a6",
   "metadata": {},
   "source": [
    "### Get the GBA template folder\n",
    "\n",
    "To test your exported model, a ready-to-use GBA project folder is provided.  \n",
    "This folder contains everything needed to build and run your model on the emulator.\n",
    "\n",
    "(and also on the real hardare !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c54d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./gba'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExportBaseGba.copy_gba_folder(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05973f",
   "metadata": {},
   "source": [
    "### Generating random inputs\n",
    "\n",
    "To test our exported model, we generate random inputs and run inference using both ONNX Runtime and the exported GBA model.\n",
    "\n",
    "Since the exported model uses full int8 and the ONNX model uses QDQ pairs, their input and output formats differ.  \n",
    "To compare results accurately, we need to retrieve the input and output scaling factors and use them to convert values between formats."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
   "id": "79c54d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fusion_passes(\n",
    "    model_path,\n",
    "    output_path=None,\n",
    "    use_gemm_fusion=True,\n",
    "    use_delete_pass=True,\n",
    "    use_delete_first_last_pass=False,\n",
    "    use_delete_first_pass=True,\n",
    "):\n",
    "    \"\"\"Apply fusion and optimization passes to an ONNX model.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = model_path\n",
    "\n",
    "    onnx_model = onnx.load(model_path)\n",
    "\n",
    "    if use_gemm_fusion:\n",
    "        fusion_pass = GemmQuantDequantFusionPass()\n",
    "        fusion_pass.run(onnx_model.graph)\n",
    "\n",
    "    if use_delete_pass:\n",
    "        delete_pass = DeleteQuantizePass()\n",
    "        delete_pass.run(onnx_model.graph)\n",
    "        \n",
    "    if use_delete_first_last_pass:\n",
    "        delete_first_pass = DeleteFirstLastQuantizeDequantizePass()\n",
    "        delete_first_pass.run(onnx_model.graph)\n",
    "\n",
    "    if use_delete_first_pass:\n",
    "        delete_first_pass = DeleteFirstInputQDQPass()\n",
    "        delete_first_pass.run(onnx_model.graph)\n",
    "\n",
    "    onnx.save(onnx_model, output_path)\n",
    "    return output_path"
>>>>>>> e8d0303d4 ([chore] moved to examples.py)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "90cf97c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
=======
=======
   "execution_count": 27,
>>>>>>> 282c52cbb ([chore] full tutorial)
=======
   "execution_count": 121,
>>>>>>> 61e1baa5c ([chore] end of the tutorial)
   "id": "844b2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = onnx.load(quantized_onnx_path)\n",
    "output_scale = OnnxUtils.get_last_qdq_scaling_factor(quantized_model.graph)[0]\n",
    "input_scale = OnnxUtils.get_first_qdq_scaling_factor(quantized_model.graph)[0]\n",
    "\n",
    "input_random = np.random.uniform(-1, 1, (1, 360)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad65c1c",
   "metadata": {},
   "source": [
    "Next, we run inference with ONNX Runtime to get the model outputs.  \n",
    "This lets us compare the results from the quantized ONNX model with those from the exported GBA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4be48a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime output: [[132.09496    -4.9847155 154.52618   -19.938862  239.26634   184.43448\n",
      "    2.4923577 107.17138   -67.293655  -99.694305 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ort_session = ort.InferenceSession(quantized_onnx_path)\n",
    "input_type = ort_session.get_inputs()[0].type\n",
    "if 'int8' in input_type:\n",
    "    model_input = input_random\n",
    "else:\n",
    "    model_input = input_random.astype(np.float32)\n",
    "ort_outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: model_input})\n",
    "onnx_output = ort_outputs[0]\n",
    "print(\"ONNX Runtime output:\", onnx_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4cb54",
   "metadata": {},
   "source": [
    "## Fuse and delete nodes\n",
    "\n",
    "To make the model compatible with our export module, we need to fuse QDQ pairs and Gemm operations.\n",
    "\n",
    "This process creates a custom QGemm node, simplifying the graph and making it easier to generate the corresponding C code for the GBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fbcbc77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting QuantizeLinear -> DequantizeLinear pair: input_QuantizeLinear -> input_DequantizeLinear\n",
      "Deleting QuantizeLinear -> DequantizeLinear pair: /Relu_output_0_QuantizeLinear -> /Relu_output_0_DequantizeLinear\n",
      "Remapped input: input_DequantizeLinear_Output -> input in node /MatMul/MatMulAddFusion_fused\n",
      "Remapped input: /Relu_output_0_DequantizeLinear_Output -> /Relu_output_0 in node /MatMul_1/MatMulAddFusion_fused\n",
      "graph main_graph (\n",
      "  %input[FLOAT, 1x360]\n",
      ") initializers (\n",
      "  %input_zero_point[INT8, scalar]\n",
      "  %input_scale[FLOAT, scalar]\n",
      "  %/Add_output_0_zero_point[INT8, scalar]\n",
      "  %/Add_output_0_scale[FLOAT, scalar]\n",
      "  %w0_zero_point[INT8, scalar]\n",
      "  %w0_scale[FLOAT, scalar]\n",
      "  %w0_quantized[INT8, 360x128]\n",
      "  %/Relu_output_0_zero_point[INT8, scalar]\n",
      "  %/Relu_output_0_scale[FLOAT, scalar]\n",
      "  %output_zero_point[INT8, scalar]\n",
      "  %output_scale[FLOAT, scalar]\n",
      "  %w1_zero_point[INT8, scalar]\n",
      "  %w1_scale[FLOAT, scalar]\n",
      "  %w1_quantized[INT8, 128x10]\n",
      "  %b0_quantized[INT32, 128]\n",
      "  %b0_quantized_scale[FLOAT, 1]\n",
      "  %b0_quantized_zero_point[INT32, scalar]\n",
      "  %b1_quantized[INT32, 10]\n",
      "  %b1_quantized_scale[FLOAT, 1]\n",
      "  %b1_quantized_zero_point[INT32, scalar]\n",
      ") {\n",
      "  %b0 = DequantizeLinear(%b0_quantized, %b0_quantized_scale, %b0_quantized_zero_point)\n",
      "  %b1 = DequantizeLinear(%b1_quantized, %b1_quantized_scale, %b1_quantized_zero_point)\n",
      "  %w0_DequantizeLinear_Output = DequantizeLinear(%w0_quantized, %w0_scale, %w0_zero_point)\n",
      "  %w1_DequantizeLinear_Output = DequantizeLinear(%w1_quantized, %w1_scale, %w1_zero_point)\n",
      "  %/Relu_output_0 = Relu(%/Add_output_0_DequantizeLinear_Output)\n",
      "  %/Add_output_0_DequantizeLinear_Output = QGemmCustom[alpha = 1, beta = 1, transA = 0, transB = 0](%input, %w0_DequantizeLinear_Output, %b0, %/Add_output_0_scale, %/Add_output_0_zero_point, %/Add_output_0_scale, %/Add_output_0_zero_point)\n",
      "  %output = QGemmCustom[alpha = 1, beta = 1, transA = 0, transB = 0](%/Relu_output_0, %w1_DequantizeLinear_Output, %b1, %output_scale, %output_zero_point, %output_scale, %output_zero_point)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fusion_pass = GemmQuantDequantFusionPass()\n",
    "delete_pass = DeleteQuantizePass()\n",
    "\n",
    "fusion_pass.run(quantized_model.graph)\n",
    "delete_pass.run(quantized_model.graph)\n",
    "onnx.save(quantized_model, fused_path)\n",
    "\n",
    "fused_model = onnx.load(fused_path)\n",
    "print(onnx.helper.printable_graph(fused_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4ac18",
   "metadata": {},
   "source": [
    "## Generate the export\n",
    "\n",
    "To generate the export, simply call the export function.  \n",
    "This will create the necessary header files containing all model parameters (weights and biases), and a `forward.c` file that implements the model’s inference logic for the GBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "af95709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported QGemm layer parameters for _MATMUL_MATMULADDFUSION_FUSED\n",
      "Exported layer _MATMUL_MATMULADDFUSION_FUSED parameters to gba/include\n",
      "Exported layer _RELU parameters to gba/include\n",
      "Exported QGemm layer parameters for _MATMUL_1_MATMULADDFUSION_FUSED\n",
      "Exported layer _MATMUL_1_MATMULADDFUSION_FUSED parameters to gba/include\n",
      "Forward function exported to: gba/source/forward.c\n",
      "Forward function header exported to: gba/include/forward.h\n"
     ]
    }
   ],
   "source": [
    "exporter = ONNXExporter(fused_path)\n",
    "exporter.export(output_dir=\"gba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31edf2d",
   "metadata": {},
   "source": [
    "### Compile it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d1d37489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba'\n",
      "forward.c\n",
      "template.c\n",
      "linking cartridge\n",
      "built ... gba.gba\n",
      "ROM fixed!\n",
      "make: Leaving directory '/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/forward.h:5,\n",
      "                 from /home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/source/forward.c:5:\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/nn_functions.h:28:24: warning: 'weight_buffer' defined but not used [-Wunused-variable]\n",
      "   28 | IN_IWRAM static int8_t weight_buffer[WEIGHT_BUFFER_BYTES] __attribute__((aligned(4)));\n",
      "      |                        ^~~~~~~~~~~~~\n",
      "In file included from /home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/source/template.c:9:\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/tests.h: In function 'all_tests':\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/tests.h:428:9: warning: variable 'start_time' set but not used [-Wunused-but-set-variable]\n",
      "  428 |     u32 start_time, end_time, result;\n",
      "      |         ^~~~~~~~~~\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/source/template.c: In function 'main':\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/source/template.c:41:17: warning: passing argument 1 of 'forward' discards 'volatile' qualifier from pointer target type [-Wdiscarded-qualifiers]\n",
      "   41 |         forward(input, output);\n",
      "      |                 ^~~~~\n",
      "In file included from /home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/source/template.c:13:\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/forward.h:7:22: note: expected 'int8_t *' {aka 'signed char *'} but argument is of type 'volatile int8_t *' {aka 'volatile signed char *'}\n",
      "    7 | void forward(int8_t *input, int8_t *output);\n",
      "      |              ~~~~~~~~^~~~~\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/source/template.c:41:24: warning: passing argument 2 of 'forward' discards 'volatile' qualifier from pointer target type [-Wdiscarded-qualifiers]\n",
      "   41 |         forward(input, output);\n",
      "      |                        ^~~~~~\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/forward.h:7:37: note: expected 'int8_t *' {aka 'signed char *'} but argument is of type 'volatile int8_t *' {aka 'volatile signed char *'}\n",
      "    7 | void forward(int8_t *input, int8_t *output);\n",
      "      |                             ~~~~~~~~^~~~~~\n",
      "In file included from /home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/forward.h:5:\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/nn_functions.h: At top level:\n",
      "/home/wboussella/Documents/rl_new_pokemon_ai/rl_new_pokemon_ai/examples/gba/include/nn_functions.h:28:24: warning: 'weight_buffer' defined but not used [-Wunused-variable]\n",
      "   28 | IN_IWRAM static int8_t weight_buffer[WEIGHT_BUFFER_BYTES] __attribute__((aligned(4)));\n",
      "      |                        ^~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"make -C gba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b171dcf",
   "metadata": {},
   "source": [
    "## Communicate with the rust emulator\n",
    "\n",
    "To run the emulator, we need to set up stop addresses, write the input data, and read the output.  \n",
    "For a detailed explanation of how this works, check out our tutorial:\n",
    "\n",
    "https://github.com/wissammm/PkmnRLArena/wiki/How-stopHandleTurn-works\n",
    "\n",
    "```c\n",
    "volatile u16 stopWriteData IN_EWRAM;\n",
    "volatile u16 stopReadData IN_EWRAM;\n",
    "volatile int8_t input[1024] IN_EWRAM;\n",
    "volatile int8_t output[10] IN_EWRAM;\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    ...\n",
    "    stopWriteData = 1;\n",
    "    forward(input, output);\n",
    "    stopReadData = 1;\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8862023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gba_environment(rom_path, map_path):\n",
    "    \"\"\"Setup GBA environment and return necessary objects.\"\"\"\n",
    "    gba = rustboyadvance_py.RustGba()\n",
    "    gba.load(PATHS[\"BIOS\"], rom_path)\n",
    "    parser = MapAnalyzer(map_path)\n",
    "    addr_write = int(parser.get_address(\"stopWriteData\"), 16)\n",
    "    addr_read = int(parser.get_address(\"stopReadData\"), 16)\n",
    "    gba.add_stop_addr(addr_write, 1, True, \"stopWriteData\", 3)\n",
    "    gba.add_stop_addr(addr_read, 1, True, \"stopReadData\", 4)\n",
    "\n",
    "    output_addr = int(parser.get_address(\"output\"), 16)\n",
    "    input_addr = int(parser.get_address(\"input\"), 16)\n",
    "\n",
    "    return gba, parser, addr_write, addr_read, output_addr, input_addr\n",
    "\n",
    "\n",
    "def run_gba_inference(\n",
    "    gba, addr_write, input_addr, output_addr, input_data, output_size\n",
    "):\n",
    "    \"\"\"Run inference on GBA and return results.\"\"\"\n",
    "    # Wait for initial stop\n",
    "    id = gba.run_to_next_stop(20000)\n",
    "    while id != 3:\n",
    "        id = gba.run_to_next_stop(20000)\n",
    "\n",
    "    # Write input data\n",
    "    gba.write_i8_list(input_addr, input_data.reshape(-1).tolist())\n",
    "    gba.write_u16(addr_write, 0)\n",
    "\n",
    "    # Wait for computation to complete\n",
    "    id = gba.run_to_next_stop(20000)\n",
    "    while id != 4:\n",
    "        id = gba.run_to_next_stop(20000)\n",
    "\n",
    "    # Read output\n",
    "    output_read = gba.read_i8_list(output_addr, output_size)\n",
    "    return np.array(output_read, dtype=np.int8).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding stop address: addr=33554976, value=1, is_active=true, name=stopWriteData, id=3\n",
      "Adding stop address: addr=33554978, value=1, is_active=true, name=stopReadData, id=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;208mWARN\u001b[0m [rustboyadvance_utils::elf] \u001b[1;38;5;208mELF: skipping program header ProgramHeader { p_type: \"PT_LOAD\", p_flags: 0x6, p_offset: 0xb8, p_vaddr: 0x30000b8, p_paddr: 0x30000b8, p_filesz: 0x0, p_memsz: 0x464, p_align: 4096 }\u001b[0m\n",
      "INFO [rustboyadvance_utils::elf] ELF: loading segment phdr: ProgramHeader { p_type: \"PT_LOAD\", p_flags: 0x5, p_offset: 0x1000, p_vaddr: 0x8000000, p_paddr: 0x8000000, p_filesz: 0x22b50, p_memsz: 0x22b50, p_align: 4096 } range 0x1000..0x23b50 vec range 0x8000000..0x8022b50\n",
      "INFO [rustboyadvance_utils::elf] ELF: loading segment phdr: ProgramHeader { p_type: \"PT_LOAD\", p_flags: 0x5, p_offset: 0x24000, p_vaddr: 0x3000000, p_paddr: 0x8022b50, p_filesz: 0xb8, p_memsz: 0xb8, p_align: 4096 } range 0x24000..0x240b8 vec range 0x8022b50..0x8022c08\n",
      "INFO [rustboyadvance_utils::elf] ELF: loading segment phdr: ProgramHeader { p_type: \"PT_LOAD\", p_flags: 0x6, p_offset: 0x2451c, p_vaddr: 0x300051c, p_paddr: 0x8022c08, p_filesz: 0x176c, p_memsz: 0x176c, p_align: 4096 } range 0x2451c..0x25c88 vec range 0x8022c08..0x8024374\n",
      "INFO [rustboyadvance_utils::elf] ELF: loading segment phdr: ProgramHeader { p_type: \"PT_LOAD\", p_flags: 0x6, p_offset: 0x26000, p_vaddr: 0x2000000, p_paddr: 0x8024374, p_filesz: 0x630, p_memsz: 0x630, p_align: 4096 } range 0x26000..0x26630 vec range 0x8024374..0x80249a4\n",
      "INFO [rustboyadvance_utils::elf] ELF: loading segment phdr: ProgramHeader { p_type: \"PT_LOAD\", p_flags: 0x4, p_offset: 0x26630, p_vaddr: 0x2000630, p_paddr: 0x80249a4, p_filesz: 0x8, p_memsz: 0x8, p_align: 4096 } range 0x26630..0x26638 vec range 0x80249a4..0x80249ac\n",
      "INFO [rustboyadvance_core::cartridge::builder] Loaded ROM: CartridgeHeader { game_title: \"\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", game_code: \"\\0\\0\\0\\0\", maker_code: \"01\", software_version: 0, checksum: 240 }\n",
      "\u001b[1;38;5;208mWARN\u001b[0m [rustboyadvance_core::cartridge::builder] \u001b[1;38;5;208mcould not detect backup save type\u001b[0m\n",
      "\u001b[1;38;5;208mWARN\u001b[0m [rustboyadvance_core::gba] \u001b[1;38;5;208mThis is not the real bios rom, some games may not be compatible\u001b[0m\n",
      "INFO [rustboyadvance_core::sound] bias - setting sample frequency to 32768hz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gba, parser, addr_write, addr_read, output_addr, input_addr = (\n",
    "    setup_gba_environment(\"gba/gba.elf\", \"gba/build/gba.map\")\n",
    ")\n",
    "# Scale and convert input to int8\n",
    "input_gba = np.round(input_random / input_scale).astype(np.int8)\n",
    "gba_output = run_gba_inference(gba, addr_write, input_addr, output_addr, \n",
    "                            input_gba, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940993a3",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Finally, we compare the outputs from ONNX Runtime and the GBA model. \n",
    "\n",
    "Since both outputs are in different formats, we dequantize them using the scaling factors to bring them to the same range.  \n",
    "This allows us to check if the results match (within a reasonable tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c426ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX output (float):[132.09496    -4.9847155 154.52618   -19.938862  239.26634   184.43448\n",
      "   2.4923577 107.17138   -67.293655  -99.694305 ]\n",
      "GBA output (float):    [127.110245  -7.477073 127.110245 -19.938862 236.77399  179.44975\n",
      "   0.       104.67902  -64.8013   -94.709595]\n",
      "Dequantized outputs match with (a lot of) tolerance!\n"
     ]
    }
   ],
   "source": [
    "onnx_float = onnx_output.astype(np.float32).reshape(-1)\n",
    "gba_float = (gba_output.astype(np.float32)) * output_scale\n",
    "\n",
    "print(f\"ONNX output (float):{onnx_float}\")\n",
    "print(f\"GBA output (float):    {gba_float}\")\n",
    "float_match = np.allclose(onnx_float, gba_float, rtol=1e-1, atol=6e4)\n",
    "if float_match:\n",
    "    print(\"Dequantized outputs match with (a lot of) tolerance!\")\n",
    "else:\n",
    "    print(\"Outputs do not match :O\")\n"
   ]
>>>>>>> e8d0303d4 ([chore] moved to examples.py)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkmn-rl-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
