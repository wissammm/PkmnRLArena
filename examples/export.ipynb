{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51769646",
   "metadata": {},
   "source": [
    "\n",
    "# Turn Your AI Into GBA Code!\n",
    "\n",
    "Ever dreamed of running your own neural network on a real Game Boy Advance? That’s exactly what this project lets you do—convert your ONNX model into C code that runs right inside Pokémon Emerald!\n",
    "\n",
    "Last time, we trained a model. Now, let’s bring it to life on the GBA.\n",
    "\n",
    "## What You’ll Do in This Tutorial \n",
    "- Load your trained ONNX model\n",
    "- Quantize it to full int8 for retro hardware\n",
    "- Run and test it directly in the emulator\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnx import shape_inference\n",
    "from onnx import numpy_helper\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pkmn_rl_arena.quantize.quantize import FullQuantizer\n",
    "from pkmn_rl_arena.export.onnx_exporter import ONNXExporter\n",
    "from pkmn_rl_arena.paths import PATHS\n",
    "\n",
    "from pkmn_rl_arena.export.passes.delete_pass import (\n",
    "    DeleteFirstInputQDQPass,\n",
    "    DeleteQuantizePass,\n",
    "    DeleteFirstLastQuantizeDequantizePass,\n",
    ")\n",
    "from pkmn_rl_arena.export.passes.fusion_pass import (\n",
    "    GemmQuantDequantFusionPass,\n",
    ")\n",
    "\n",
    "from pkmn_rl_arena.export.base import ExportBaseGba\n",
    "from pkmn_rl_arena.export.onnx_utils import OnnxUtils\n",
    "from pkmn_rl_arena.data.parser import MapAnalyzer\n",
    "\n",
    "from pkmn_rl_arena.export.exporters.parameters import ExportParameters\n",
    "import rustboyadvance_py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe0918",
   "metadata": {},
   "source": [
    "### Set Model Paths\n",
    "\n",
    "Before moving forward, let's organize the three key ONNX files we'll use:\n",
    "\n",
    "- **Original ONNX:** The model you trained, in full precision.\n",
    "- **Quantized ONNX:** Converted to int8 for hardware compatibility, and used to test inference with ONNX Runtime.\n",
    "- **Fused ONNX:** Created from the quantized model, with Gemm and QDQ operations fused together so our export module can process it for the GBA.\n",
    "\n",
    "Defining these paths now will make the next steps much smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4990bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"pokemon_battle_model.onnx\"\n",
    "quantized_onnx_path = \"pokemon_battle_model_quantized.onnx\"\n",
    "fused_path = \"pokemon_battle_model_quantized_fused.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf4b16",
   "metadata": {},
   "source": [
    "You can print the actual graph with ONNX to see the changes we make.  \n",
    "For a clearer visualization, try Netron—a handy tool for exploring ONNX model graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_raw = onnx.load(onnx_path)\n",
    "print(onnx.helper.printable_graph(onnx_raw.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964fd6b",
   "metadata": {},
   "source": [
    "## Quantize the model\n",
    "\n",
    "Quantization is the process of converting your model’s weights and activations from floating point to int8, making it much more efficient for hardware like the GBA.  \n",
    "\n",
    "To do this, we need to calibrate QDQ (Quantize-Dequantize) pairs using sample data, which helps the model learn how to scale values correctly.\n",
    "\n",
    "> Note:  \n",
    "> Calibration works best with real data from your application. In future versions, we’ll provide a tool to help generate calibration data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec996992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_samples = 10\n",
    "\n",
    "quantizer = FullQuantizer(onnx_path, quantized_onnx_path)\n",
    "calib_reader = FullQuantizer.create_fake_calibration_data(\n",
    "    onnx_path, num_samples=num_samples\n",
    ")\n",
    "quantizer.quantize(calib_reader)\n",
    "\n",
    "# Infer shapes\n",
    "quantized_model = onnx.load(quantized_onnx_path)\n",
    "inferred_model = shape_inference.infer_shapes(quantized_model)\n",
    "onnx.save(inferred_model, quantized_onnx_path)\n",
    "\n",
    "print(onnx.helper.printable_graph(inferred_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f214a6",
   "metadata": {},
   "source": [
    "### Get the GBA template folder\n",
    "\n",
    "To test your exported model, a ready-to-use GBA project folder is provided.  \n",
    "This folder contains everything needed to build and run your model on the emulator.\n",
    "\n",
    "(and also on the real hardare !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c54d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExportBaseGba.copy_gba_folder(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05973f",
   "metadata": {},
   "source": [
    "### Generating random inputs\n",
    "\n",
    "To test our exported model, we generate random inputs and run inference using both ONNX Runtime and the exported GBA model.\n",
    "\n",
    "Since the exported model uses full int8 and the ONNX model uses QDQ pairs, their input and output formats differ.  \n",
    "To compare results accurately, we need to retrieve the input and output scaling factors and use them to convert values between formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = onnx.load(quantized_onnx_path)\n",
    "output_scale = OnnxUtils.get_last_qdq_scaling_factor(quantized_model.graph)[0]\n",
    "input_scale = OnnxUtils.get_first_qdq_scaling_factor(quantized_model.graph)[0]\n",
    "\n",
    "input_random = np.random.uniform(-1, 1, (1, 360)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad65c1c",
   "metadata": {},
   "source": [
    "Next, we run inference with ONNX Runtime to get the model outputs.  \n",
    "This lets us compare the results from the quantized ONNX model with those from the exported GBA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be48a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ort_session = ort.InferenceSession(quantized_onnx_path)\n",
    "input_type = ort_session.get_inputs()[0].type\n",
    "if 'int8' in input_type:\n",
    "    model_input = input_random\n",
    "else:\n",
    "    model_input = input_random.astype(np.float32)\n",
    "ort_outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: model_input})\n",
    "onnx_output = ort_outputs[0]\n",
    "print(\"ONNX Runtime output:\", onnx_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4cb54",
   "metadata": {},
   "source": [
    "## Fuse and delete nodes\n",
    "\n",
    "To make the model compatible with our export module, we need to fuse QDQ pairs and Gemm operations.\n",
    "\n",
    "This process creates a custom QGemm node, simplifying the graph and making it easier to generate the corresponding C code for the GBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_pass = GemmQuantDequantFusionPass()\n",
    "delete_pass = DeleteQuantizePass()\n",
    "\n",
    "fusion_pass.run(quantized_model.graph)\n",
    "delete_pass.run(quantized_model.graph)\n",
    "onnx.save(quantized_model, fused_path)\n",
    "\n",
    "fused_model = onnx.load(fused_path)\n",
    "print(onnx.helper.printable_graph(fused_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4ac18",
   "metadata": {},
   "source": [
    "## Generate the export\n",
    "\n",
    "To generate the export, simply call the export function.  \n",
    "This will create the necessary header files containing all model parameters (weights and biases), and a `forward.c` file that implements the model’s inference logic for the GBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter = ONNXExporter(fused_path)\n",
    "exporter.export(output_dir=\"gba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31edf2d",
   "metadata": {},
   "source": [
    "### Compile it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d37489",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"make -C gba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b171dcf",
   "metadata": {},
   "source": [
    "## Communicate with the rust emulator\n",
    "\n",
    "To run the emulator, we need to set up stop addresses, write the input data, and read the output.  \n",
    "For a detailed explanation of how this works, check out our tutorial:\n",
    "\n",
    "https://github.com/wissammm/PkmnRLArena/wiki/How-stopHandleTurn-works\n",
    "\n",
    "```c\n",
    "volatile u16 stopWriteData IN_EWRAM;\n",
    "volatile u16 stopReadData IN_EWRAM;\n",
    "volatile int8_t input[1024] IN_EWRAM;\n",
    "volatile int8_t output[10] IN_EWRAM;\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    ...\n",
    "    stopWriteData = 1;\n",
    "    forward(input, output);\n",
    "    stopReadData = 1;\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gba_environment(rom_path, map_path):\n",
    "    \"\"\"Setup GBA environment and return necessary objects.\"\"\"\n",
    "    gba = rustboyadvance_py.RustGba()\n",
    "    gba.load(PATHS[\"BIOS\"], rom_path)\n",
    "    parser = MapAnalyzer(map_path)\n",
    "    addr_write = int(parser.get_address(\"stopWriteData\"), 16)\n",
    "    addr_read = int(parser.get_address(\"stopReadData\"), 16)\n",
    "    gba.add_stop_addr(addr_write, 1, True, \"stopWriteData\", 3)\n",
    "    gba.add_stop_addr(addr_read, 1, True, \"stopReadData\", 4)\n",
    "\n",
    "    output_addr = int(parser.get_address(\"output\"), 16)\n",
    "    input_addr = int(parser.get_address(\"input\"), 16)\n",
    "\n",
    "    return gba, parser, addr_write, addr_read, output_addr, input_addr\n",
    "\n",
    "\n",
    "def run_gba_inference(\n",
    "    gba, addr_write, input_addr, output_addr, input_data, output_size\n",
    "):\n",
    "    \"\"\"Run inference on GBA and return results.\"\"\"\n",
    "    # Wait for initial stop\n",
    "    id = gba.run_to_next_stop(20000)\n",
    "    while id != 3:\n",
    "        id = gba.run_to_next_stop(20000)\n",
    "\n",
    "    # Write input data\n",
    "    gba.write_i8_list(input_addr, input_data.reshape(-1).tolist())\n",
    "    gba.write_u16(addr_write, 0)\n",
    "\n",
    "    # Wait for computation to complete\n",
    "    id = gba.run_to_next_stop(20000)\n",
    "    while id != 4:\n",
    "        id = gba.run_to_next_stop(20000)\n",
    "\n",
    "    # Read output\n",
    "    output_read = gba.read_i8_list(output_addr, output_size)\n",
    "    return np.array(output_read, dtype=np.int8).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gba, parser, addr_write, addr_read, output_addr, input_addr = (\n",
    "    setup_gba_environment(\"gba/gba.elf\", \"gba/build/gba.map\")\n",
    ")\n",
    "# Scale and convert input to int8\n",
    "input_gba = np.round(input_random / input_scale).astype(np.int8)\n",
    "gba_output = run_gba_inference(gba, addr_write, input_addr, output_addr, \n",
    "                            input_gba, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940993a3",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Finally, we compare the outputs from ONNX Runtime and the GBA model. \n",
    "\n",
    "Since both outputs are in different formats, we dequantize them using the scaling factors to bring them to the same range.  \n",
    "This allows us to check if the results match (within a reasonable tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c426ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_float = onnx_output.astype(np.float32).reshape(-1)\n",
    "gba_float = (gba_output.astype(np.float32)) * output_scale\n",
    "\n",
    "print(f\"ONNX output (float):{onnx_float}\")\n",
    "print(f\"GBA output (float):    {gba_float}\")\n",
    "float_match = np.allclose(onnx_float, gba_float, rtol=1e-1, atol=6e4)\n",
    "if float_match:\n",
    "    print(\"Dequantized outputs match with (a lot of) tolerance!\")\n",
    "else:\n",
    "    print(\"Outputs do not match :O\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkmn-rl-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
