{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c4b235",
   "metadata": {},
   "source": [
    "# Our Library\n",
    "\n",
    "### Neural Network Training Library\n",
    "\n",
    "We made a Python library that lets us train AI by actually playing Pokémon! The library handles everything: watching the game, picking moves, and learning from mistakes—so the AI gets better all by itself.\n",
    "\n",
    "### Rust-based Emulator with PyO3 Bindings\n",
    "\n",
    "We souped up a super-fast Game Boy Advance emulator (written in Rust) so it can chat with Python. Thanks to PyO3, Python can now:\n",
    "- **See what’s happening:** Grab all the juicy game details—Pokémon stats, battle info, and more—and send them to the AI.\n",
    "- **Control the action:** Let the AI pick moves or switch Pokémon, and actually make those things happen in-game.\n",
    "\n",
    "\n",
    "### Custom Pokémon Disassembly for RL\n",
    "\n",
    "We also hacked the Pokémon Emerald game code so it can:\n",
    "- **Understand the AI’s commands** sent from Python.\n",
    "- **Skip all the boring stuff** (like graphics and text) so training is way, way faster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807fc9ba",
   "metadata": {},
   "source": [
    "# The tutorial \n",
    "In this tutorial, we will, step by step, train a neural network with our librairy\n",
    "## Goals : \n",
    " - Train a small model in self-play RL to be the best on 1v1 battles \n",
    " - Watch the performance of our model \n",
    " - Export the model in ONNX (needed for teh next tutorial, run the model on GBA)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0e760",
   "metadata": {},
   "source": [
    " ## Imports\n",
    " Make sure that you followed the README.md install step correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3447c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for training and interacting with the environment\n",
    "import sys\n",
    "sys.path.append(\"..\")  \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# PettingZoo for multi-agent RL environments\n",
    "from pettingzoo.utils import parallel_to_aec\n",
    "from pettingzoo.test import parallel_api_test\n",
    "\n",
    "# Main environment and core components\n",
    "from pkmn_rl_arena.env.battle_core import BattleCore\n",
    "from pkmn_rl_arena.env.battle_arena import BattleArena, RenderMode\n",
    "from pkmn_rl_arena.env.replay_buffer import ReplayBuffer\n",
    "from pkmn_rl_arena.env.pkmn_team_factory import PkmnTeamFactory\n",
    "from pkmn_rl_arena.env.observation import ObservationFactory, ObsIdx\n",
    "from pkmn_rl_arena.paths import PATHS\n",
    "\n",
    "# Logging and debugging\n",
    "from pkmn_rl_arena import log\n",
    "\n",
    "# For RL algorithms and neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "log.setLevel(\"CRITICAL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39773c6e",
   "metadata": {},
   "source": [
    "## Instanciate\n",
    "With PyTorch, we can easily create a model — and as you can see, this one is really small. But why keep it so small?\n",
    "On a regular PC, the model size doesn’t matter too much (it mostly depends on your hardware).\n",
    "However, if you want to export and run the model on the GBA, memory becomes a huge limitation. Here’s the compilation info from **pokeemerald**:\n",
    "```bash\n",
    "Memory region         Used Size  Region Size  %age Used\n",
    "           EWRAM:      251688 B       256 KB     96.01%\n",
    "           IWRAM:       30416 B        32 KB     92.82%\n",
    "             ROM:    13334028 B        32 MB     39.74%\n",
    "```\n",
    "What does this mean in practice?\n",
    "- EWRAM → about 10.2 KB left, this is your real RAM (read–write), similar to system RAM on a PC.\n",
    "- ROM → about 19.27 MB free, this is slow, read-only memory, but it’s where we can store the model’s weights.\n",
    "\n",
    "If we quantize the model to int8, then:\n",
    "- We can store up to ~20 million parameters in ROM.\n",
    "- But RAM is extremely limited: only 10,456 int8 values can fit into EWRAM.\n",
    "\n",
    "That’s why we need to be very careful. For each node n in the model, the sum of its inputs and outputs must stay below this RAM limit:\n",
    "$$\n",
    "\\text{input}_n + \\text{output}_n < 10,456\n",
    "$$\n",
    "\n",
    "![My model architecture](./assets/gba-archi-model.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51468bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "core = BattleCore(PATHS[\"ROM\"], PATHS[\"BIOS\"], PATHS[\"MAP\"])\n",
    "env = BattleArena(core)\n",
    "\n",
    "# get observation and action space sizes\n",
    "obs = env.reset()[0]\n",
    "obs_size = obs[\"player\"][\"observation\"].shape[0]\n",
    "action_size = env.action_manager.action_space_size\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Parameter(torch.randn(obs_size, 128))\n",
    "        self.b0 = nn.Parameter(torch.randn(128))\n",
    "        self.w1 = nn.Parameter(torch.randn(128, action_size))\n",
    "        self.b1 = nn.Parameter(torch.randn(action_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(x, self.w0) + self.b0\n",
    "        x = F.relu(x)\n",
    "        x = torch.matmul(x, self.w1) + self.b1\n",
    "        return x\n",
    "\n",
    "\n",
    "shared_agent = DQN(obs_size, action_size)\n",
    "optimizer = optim.Adam(shared_agent.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Environment and agents initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c98a6f",
   "metadata": {},
   "source": [
    "### Training Method\n",
    "As you know we're gonna use Self-Play to train our model, only one model is used to train both agents.\n",
    "Here's a diagram on how it works : \n",
    "\n",
    "![My model architecture](./assets/env.png)\n",
    "\n",
    "To have a fast training, in our example we're only launching 5 episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10536709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99  # discount factor\n",
    "EPSILON_START = 1.0  # initial exploration rate\n",
    "EPSILON_END = 0.1  # final exploration rate\n",
    "EPSILON_DECAY = 0.95  # decay rate per episode\n",
    "BATCH_SIZE = 64  # batch size for replay buffer\n",
    "NUM_EPISODES = 3  # number of episodes to train\n",
    "TARGET_UPDATE = 10  # update target network every N episodes\n",
    "\n",
    "target_network = DQN(obs_size, action_size)\n",
    "target_network.load_state_dict(shared_agent.state_dict())\n",
    "\n",
    "# For tracking performance\n",
    "rewards_history = []\n",
    "win_rates = []\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334aa019",
   "metadata": {},
   "source": [
    "### Action Selection and Model Optimization\n",
    "**Action Selection (** select_action **)**\n",
    "\n",
    "This function implements the classic epsilon-greedy strategy for reinforcement learning:\n",
    "\n",
    "- With probability epsilon, the agent chooses a random valid action (exploration).\n",
    "- Otherwise, it selects the action with the highest predicted Q-value from the neural network (exploitation).\n",
    "- The action mask ensures only legal actions are considered.\n",
    "\n",
    "**Model Optimization (** optimize_model **)**\n",
    "\n",
    "This function updates the neural network using experiences sampled from the replay buffer:\n",
    "\n",
    "- A random batch of transitions is sampled.\n",
    "- The network predicts Q-values for the current and next states.\n",
    "- The loss is computed using the Bellman equation and mean squared error.\n",
    "- The optimizer updates the network weights to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b749641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(agent, state, epsilon):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        # Random action (exploration)\n",
    "        valid_actions = np.where(state[\"action_mask\"] == 1)[0]\n",
    "        if len(valid_actions) > 0:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            return random.randint(0, action_size - 1)\n",
    "    else:\n",
    "        # Greedy action (exploitation)\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state[\"observation\"])\n",
    "            q_values = shared_agent(state_tensor)\n",
    "            \n",
    "            # Apply action mask to only consider valid actions\n",
    "            mask = torch.FloatTensor(state[\"action_mask\"])\n",
    "            q_values = q_values * mask - 1000.0 * (1 - mask)\n",
    "            \n",
    "            return q_values.argmax().item()\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Update neural network from experiences in replay buffer\"\"\"\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return 0\n",
    "\n",
    "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "    states = torch.FloatTensor(np.array([t[0][\"observation\"] for t in transitions]))\n",
    "    actions = torch.LongTensor([t[1] for t in transitions]).unsqueeze(1)\n",
    "    rewards = torch.FloatTensor([t[2] for t in transitions])\n",
    "    next_states = torch.FloatTensor(np.array([t[3][\"observation\"] for t in transitions]))\n",
    "    dones = torch.FloatTensor([t[4] for t in transitions])\n",
    "\n",
    "    current_q = shared_agent(states).gather(1, actions).squeeze(1)\n",
    "    with torch.no_grad():\n",
    "        max_next_q = target_network(next_states).max(1)[0]\n",
    "        target_q = rewards + GAMMA * max_next_q * (1 - dones)\n",
    "\n",
    "    loss = nn.MSELoss()(current_q, target_q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b11a16",
   "metadata": {},
   "source": [
    "### Training\n",
    "**Training Loop:** Where the Magic (and RNG) Happens\n",
    "\n",
    "This is where our neural network gets its hands dirty—by battling itself, over and over, until it (hopefully) learns something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d126a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"Starting training for {NUM_EPISODES} episodes\")\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Reset environment\n",
    "    observations, _ = env.reset()\n",
    "    episode_rewards = {\"player\": 0, \"enemy\": 0}\n",
    "    done = False\n",
    "    \n",
    "    # Episode loop\n",
    "    while not done:\n",
    "        # Select actions for both agents\n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            actions[agent] = select_action(agent, observations[agent], epsilon)\n",
    "        \n",
    "        # Take a step in the environment\n",
    "        next_observations, rewards, terminations, truncations, _ = env.step(actions)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "        \n",
    "        # Store transitions in replay buffers\n",
    "        for agent in env.agents:\n",
    "            replay_buffer.push((\n",
    "                observations[agent],\n",
    "                actions[agent],\n",
    "                rewards[agent],\n",
    "                next_observations[agent],\n",
    "                done\n",
    "            ))\n",
    "                    \n",
    "            # Track rewards\n",
    "            episode_rewards[agent] += rewards[agent]\n",
    "        \n",
    "        # Update observations\n",
    "        observations = next_observations\n",
    "        \n",
    "        # Optimize the model\n",
    "        loss = optimize_model()\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "    \n",
    "    # Update target network periodically\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_network.load_state_dict(shared_agent.state_dict())\n",
    "    \n",
    "    # Track rewards\n",
    "    total_reward = sum(episode_rewards.values())\n",
    "    rewards_history.append(total_reward)\n",
    "    \n",
    "    # Calculate win rate (who won more often)\n",
    "    if episode_rewards[\"player\"] > episode_rewards[\"enemy\"]:\n",
    "        win_rates.append(1)  # Player won\n",
    "    elif episode_rewards[\"player\"] < episode_rewards[\"enemy\"]:\n",
    "        win_rates.append(0)  # Enemy won\n",
    "    else:\n",
    "        win_rates.append(0.5)  # Draw\n",
    "    \n",
    "    # Log progress\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(rewards_history[-10:]) if len(rewards_history) >= 10 else np.mean(rewards_history)\n",
    "        win_rate = np.mean(win_rates[-10:]) if len(win_rates) >= 10 else np.mean(win_rates)\n",
    "        print(f\"Episode {episode}/{NUM_EPISODES}, Avg Reward: {avg_reward:.2f}, Win Rate: {win_rate:.2f}, Epsilon: {epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cd3fb",
   "metadata": {},
   "source": [
    "### Export\n",
    "Once our model is trained, we can export the model to run it inside a GBA\n",
    "We see you in the next tutorial `export.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model to ONNX format\n",
    "print(\"Exporting model to ONNX format...\")\n",
    "dummy_input = torch.randn(1, obs_size)\n",
    "torch.onnx.export(\n",
    "    shared_agent,\n",
    "    dummy_input,\n",
    "    \"pokemon_battle_model.onnx\",\n",
    "    opset_version=11,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    do_constant_folding=True,\n",
    "    training=torch.onnx.TrainingMode.EVAL,\n",
    ")\n",
    "print(\"Model exported successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkmn-rl-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
