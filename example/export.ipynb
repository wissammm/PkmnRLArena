{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51769646",
   "metadata": {},
   "source": [
    "# Generate a C export for the GBA\n",
    "If you have trained model in onnx, you can generate a c export of inference for the gba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3e066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnx import shape_inference\n",
    "from onnx import numpy_helper\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pkmn_rl_arena.quantize.quantize import FullQuantizer\n",
    "from pkmn_rl_arena.export.onnx_exporter import ONNXExporter\n",
    "from pkmn_rl_arena.paths import PATHS\n",
    "\n",
    "from pkmn_rl_arena.export.passes.delete_pass import (\n",
    "    DeleteFirstInputQDQPass,\n",
    "    DeleteQuantizePass,\n",
    "    DeleteFirstLastQuantizeDequantizePass,\n",
    ")\n",
    "from pkmn_rl_arena.export.passes.fusion_pass import (\n",
    "    GemmQuantDequantFusionPass,\n",
    ")\n",
    "\n",
    "from pkmn_rl_arena.export.base import ExportBaseGba\n",
    "\n",
    "from pkmn_rl_arena.export.exporters.parameters import ExportParameters\n",
    "import rustboyadvance_py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4990bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./gba'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "onnx_path = \"pokemon_battle_model.onnx\"\n",
    "quantized_onnx_path = \"pokemon_battle_model_quantized.onnx\"\n",
    "fused_path = \"pokemon_battle_model_quantized_fused.onnx\"\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "quantizer = FullQuantizer(onnx_path, quantized_onnx_path)\n",
    "calib_reader = FullQuantizer.create_fake_calibration_data(\n",
    "    onnx_path, num_samples=num_samples\n",
    ")\n",
    "quantizer.quantize(calib_reader)\n",
    "\n",
    "# Infer shapes\n",
    "quantized_model = onnx.load(quantized_onnx_path)\n",
    "inferred_model = shape_inference.infer_shapes(quantized_model)\n",
    "onnx.save(inferred_model, quantized_onnx_path)\n",
    "\n",
    "ExportBaseGba.copy_gba_folder(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec996992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_qdq_scaling_factor(graph):\n",
    "    \"\"\"\n",
    "    Get the actual scale and zero_point value of the last DequantizeLinear node before output.\n",
    "    Returns (scale_value, zero_point_value)\n",
    "    \"\"\"\n",
    "    for node in reversed(graph.node):\n",
    "        if node.op_type == \"DequantizeLinear\":\n",
    "            # Find scale and zero_point names\n",
    "            scale_name = node.input[1]\n",
    "            zero_point_name = node.input[2]\n",
    "            scale_value = None\n",
    "            zero_point_value = None\n",
    "            # Search initializers for actual values\n",
    "            for init in graph.initializer:\n",
    "                if init.name == scale_name:\n",
    "                    scale_value = float(numpy_helper.to_array(init))\n",
    "                if init.name == zero_point_name:\n",
    "                    zero_point_value = int(numpy_helper.to_array(init))\n",
    "            if scale_value is not None and zero_point_value is not None:\n",
    "                return scale_value, zero_point_value\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_first_qdq_scaling_factor(graph):\n",
    "    \"\"\"\n",
    "    Get the actual scale and zero_point value of the first QuantizeLinear node after input.\n",
    "    Returns (scale_value, zero_point_value)\n",
    "    \"\"\"\n",
    "    for node in graph.node:\n",
    "        if node.op_type == \"QuantizeLinear\":\n",
    "            # Find scale and zero_point names\n",
    "            scale_name = node.input[1]\n",
    "            zero_point_name = node.input[2]\n",
    "            scale_value = None\n",
    "            zero_point_value = None\n",
    "            # Search initializers for actual values\n",
    "            for init in graph.initializer:\n",
    "                if init.name == scale_name:\n",
    "                    scale_value = float(numpy_helper.to_array(init))\n",
    "                if init.name == zero_point_name:\n",
    "                    zero_point_value = int(numpy_helper.to_array(init))\n",
    "            if scale_value is not None and zero_point_value is not None:\n",
    "                return scale_value, zero_point_value\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c54d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fusion_passes(\n",
    "    model_path,\n",
    "    output_path=None,\n",
    "    use_gemm_fusion=True,\n",
    "    use_delete_pass=True,\n",
    "    use_delete_first_last_pass=False,\n",
    "    use_delete_first_pass=True,\n",
    "):\n",
    "    \"\"\"Apply fusion and optimization passes to an ONNX model.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = model_path\n",
    "\n",
    "    onnx_model = onnx.load(model_path)\n",
    "\n",
    "    if use_gemm_fusion:\n",
    "        fusion_pass = GemmQuantDequantFusionPass()\n",
    "        fusion_pass.run(onnx_model.graph)\n",
    "\n",
    "    if use_delete_pass:\n",
    "        delete_pass = DeleteQuantizePass()\n",
    "        delete_pass.run(onnx_model.graph)\n",
    "        \n",
    "    if use_delete_first_last_pass:\n",
    "        delete_first_pass = DeleteFirstLastQuantizeDequantizePass()\n",
    "        delete_first_pass.run(onnx_model.graph)\n",
    "\n",
    "    if use_delete_first_pass:\n",
    "        delete_first_pass = DeleteFirstInputQDQPass()\n",
    "        delete_first_pass.run(onnx_model.graph)\n",
    "\n",
    "    onnx.save(onnx_model, output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b2f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting QuantizeLinear -> DequantizeLinear pair: input_QuantizeLinear -> input_DequantizeLinear\n",
      "Deleting QuantizeLinear -> DequantizeLinear pair: /net/net.1/Relu_output_0_QuantizeLinear -> /net/net.1/Relu_output_0_DequantizeLinear\n",
      "Remapped input: input_DequantizeLinear_Output -> input in node /net/net.0/Gemm_fused\n",
      "Remapped input: /net/net.1/Relu_output_0_DequantizeLinear_Output -> /net/net.1/Relu_output_0 in node /net/net.2/Gemm_fused\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m apply_fusion_passes(quantized_onnx_path, fused_path, \n\u001b[32m     15\u001b[39m                     use_gemm_fusion=\u001b[38;5;28;01mTrue\u001b[39;00m, use_delete_pass=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m     16\u001b[39m                     use_delete_first_pass=\u001b[38;5;28;01mFalse\u001b[39;00m, use_delete_first_last_pass=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     17\u001b[39m exporter = ONNXExporter(fused_path)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m exporter.export(output_dir=\u001b[43mos\u001b[49m.path.join(os.path.dirname(\u001b[34m__file__\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mtest_export_data/gba\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     19\u001b[39m launch_makefile()\n\u001b[32m     20\u001b[39m gba, parser, addr_write, addr_read, output_addr, input_addr = setup_gba_environment(\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.rom_path, \u001b[38;5;28mself\u001b[39m.map_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "quantized_model = onnx.load(quantized_onnx_path)\n",
    "output_scale = get_last_qdq_scaling_factor(quantized_model.graph)[0]\n",
    "if output_scale is None:\n",
    "    raise ValueError(\"Could not find output DQ node scale and zero point\")\n",
    "input_random = np.random.uniform(-1, 1, (1, 360)).astype(np.float32)\n",
    "ort_session = ort.InferenceSession(quantized_onnx_path)\n",
    "input_type = ort_session.get_inputs()[0].type\n",
    "if 'int8' in input_type:\n",
    "    model_input = input_random\n",
    "else:\n",
    "    model_input = input_random.astype(np.float32)\n",
    "ort_outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: model_input})\n",
    "onnx_output = ort_outputs[0]\n",
    "apply_fusion_passes(quantized_onnx_path, fused_path, \n",
    "                    use_gemm_fusion=True, use_delete_pass=True, \n",
    "                    use_delete_first_pass=False, use_delete_first_last_pass=False)\n",
    "exporter = ONNXExporter(fused_path)\n",
    "exporter.export(output_dir=\"gba\")\n",
    "launch_makefile()\n",
    "gba, parser, addr_write, addr_read, output_addr, input_addr = setup_gba_environment(\n",
    "    self.rom_path, self.map_path)\n",
    "input_scale = get_first_qdq_scaling_factor(quantized_model.graph)[0]\n",
    "input_gba = np.round(input_random / input_scale).astype(np.int8)\n",
    "gba_output = run_gba_inference(gba, addr_write, input_addr, output_addr, \n",
    "                            input_gba, 5)\n",
    "print(f\"ONNX output (int8): {onnx_output}\")\n",
    "print(f\"GBA output (int8): {gba_output}\")\n",
    "onnx_float = onnx_output.astype(np.float32)\n",
    "gba_float = (gba_output.astype(np.float32)) * output_scale\n",
    "print(f\"ONNX output (float): {onnx_float}\")\n",
    "print(f\"GBA output (float): {gba_float}\")\n",
    "float_match = np.allclose(onnx_float, gba_float, rtol=1e-1, atol=6e4)\n",
    "if float_match:\n",
    "    print(\"Dequantized outputs match within tolerance!\")\n",
    "else:\n",
    "    print(\"Dequantized outputs don't match\")\n",
    "    diff = np.abs(onnx_float - gba_float)\n",
    "    max_diff = np.max(diff)\n",
    "    avg_diff = np.mean(diff)\n",
    "    print(f\"Max difference: {max_diff}\")\n",
    "    print(f\"Average difference: {avg_diff}\")\n",
    "    print(f\"Differences: {diff}\")\n",
    "self.assertTrue(float_match, \"Outputs don't match even after dequantization\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
